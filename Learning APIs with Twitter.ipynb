{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## <center>Learning to use APIs, starting with Twitter</center>##\n",
    "<br><br>\n",
    "\n",
    "In the realm of data science, an API (_application programming interface_), is a means of connecting to a database maintained by another party. That definition may sound simultaneously too simple and/or too abstract to mean anything so let's use Facebook as an example to explain it.\n",
    "<br>\n",
    "<br>\n",
    "If you're logged into your account via the Facebook app or website, any data you create or consume will be routed through your connection to the company's servers and into or out of their oceans of data. Many would love to have be able to glimpse at the data flowing over that connection, but Facebook is making billions off of those data and doesn't want to share it all, especially because users wouldn't like that. But Facebook also knows that third-party apps add a lot of value to the platform, so they need a way to allow third parties to create new data and see some of a user's existing data on behalf of the user. The Facebook API is the means by which they do that. An API defines protocols and methods for using pretty much any machine with an internet connection to connect to and make requests of a webserver and the data storage system behind it.   \n",
    "<br>\n",
    "The term API actually covers more than what I described above. For example, if you've used `matplotlib` for data visualization in Python, you shouldn't be using the classes defined by the main package directly. You should be using `pyplot`--via a statement like `import matplotlib.pyplot as plt`--, which is an API whose functions manages a lot of the hardest work for you. But in the world of data science/big data, the term generally refers to a means to connect to a server and database.\n",
    "<br>\n",
    "<br>\n",
    "As data hungry scientists, we might wish every data storage system had an API we could plug into, but (un?)-fortunately they don't. In fact, is more likely that they don't have an API than they do. Places like the US Census Bureau have APIs as a public service, but for a company to invest in making and maintaning an API, and risk losing user's trust by making personal information available, the company needs to see a significant financial upside to it. Most companies don't, and the APIs that do exist usually don't make all data available and might in fact be trending toward making less available. This has happened with the Facebook API and for good reason. A lot of personal information was made available to third-party apps with user permissions it's clear that users aren't comfortable with that (see Cambridge Analytica).\n",
    "<br><br>\n",
    "## But then there was Twitter ##\n",
    "There is a technical reason why you've seen and heard about so much research based on Twitter data. Because the platform aims to be a place for public discussion and dissemination of information, Twitter makes its easy to algorithmically collect its data, including metadata and other things not rendered in feeds. The comparative ease with which anyone can get data make Twitter a go-to source for trying to answer some types of questions. The jury is still out on how generalizable the finding are, but President Trump's affinity for the platform has at least kept it relevant and interesting.\n",
    "<br><br>\n",
    "To introduce you to APIs, we are doing to explore Twitter data and produce some descriptive findings. To make this notebook work, I've created a module that can help gather, archive, search for and display tweets interactively. When there is a clear relationship between what the module's functions are doing and what the actual Twitter API is doing, I've included the actual API call (what the command looks like). \n",
    "<br><br>\n",
    "I've chosen this approach over dealing directly with the Twitter API because (1) the raw data you get back from the API can be overwhelming and possibly discouraging; (2) I'm trying to give you a general feel for how APIs work, not bore you with the details of the Twitter API; and (3) it's a lot of fun to play around with Twitter data and I want to make sure we get to that, which we couldn't do without sidelining some of the specific details.\n",
    "<br><br>\n",
    "### A couple more important details about using the Twitter API ###\n",
    "<br>\n",
    "First, Twitter does not provide access to all tweets with the standard connection, but instead allows users to find particular tweets or sample from realtime or historical data. The sample is random in principle, but the sampling schemes aren't public and the highly dynamic nature of the platform means there are probably a number of schemes that could be defended as being properly random. This shouldn't amount to much, but it's worth knowing.\n",
    "<br><br>\n",
    "The API Twitter provides uses the [REST protocol](https://spring.io/understanding/REST), which means if you know the right way to format the requests you can send them and get responses using just your web browser. It turns out this is clunky and inconvenient so people create libraries mapping commands written in your favorite programming language to the proper REST requests. We'll be making use of one of the Python packages, `twitter` below so you need to have it on your computer. We'll also need the package/library `whoosh` for creating a searchable index. Use the Anaconda Navigator or the command line to install the packages. The Navigator tabs for environments you can search for the packages by name and import them. Use either `easy_install twitter` or `pip install twitter` for command line installation.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Topic 1: the JSON format \n",
    "<br><br>\n",
    "Before we get to the API, we need to seque to the data format that the Twitter API, and many others, uses to give data to us. A common format for API data is the *JSON* (JavaScript Object Notation) format. A *JSON* formatted file is human-readable and ends with *.json*. At this point, the format's only meaningful connection to its namesake JavaScript is that it is built around the concept of *objects*, which is at the core of the object-oriented programming paradigm that Java helped popularized. This matters to us only in that thinking in terms of objects is helpful for a lot of big data research.\n",
    "<br><br>\n",
    "Consider a tweet:\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I had to fire General Flynn because he lied to the Vice President and the FBI. He has pled guilty to those lies. It is a shame because his actions during the transition were lawful. There was nothing to hide!</p>&mdash; Donald J. Trump (@realDonaldTrump) <a href=\"https://twitter.com/realDonaldTrump/status/937007006526959618?ref_src=twsrc%5Etfw\">December 2, 2017</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "\n",
    "<br>\n",
    "In our daily lives, we view a tweet as a (very) short piece of text but there is a lot more to a tweet. Below is the JSON representation for the same tweet. \n",
    "<br><br>\n",
    "\n",
    "<code> {'created_at': 'Sat Dec 02 17:14:13 +0000 2017',\n",
    "        'id': 937007006526959618,\n",
    "        'id_str': '937007006526959618',\n",
    "         'text': 'I had to fire General Flynn because he lied to the Vice President and the FBI. He has pled guilty to those lies. Itâ€¦ https://t.co/8bMCLGuAhf',\n",
    "         'truncated': True, \n",
    "         'entities': \n",
    "             {'hashtags': [],\n",
    "             'symbols': [],\n",
    "             'user_mentions': [],\n",
    "             'urls': [\n",
    "                {'url': 'https://t.co/8bMCLGuAhf',\n",
    "                'expanded_url': 'https://twitter.com/i/web/status/937007006526959618',\n",
    "                'display_url': 'twitter.com/i/web/status/9â€¦',\n",
    "                'indices': [117, 140]}\n",
    "              ]\n",
    "              }, \n",
    "         'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
    "         'in_reply_to_status_id': None,\n",
    "         'in_reply_to_status_id_str': None,\n",
    "         'in_reply_to_user_id': None,\n",
    "         'in_reply_to_user_id_str': None,\n",
    "         'in_reply_to_screen_name': None, \n",
    "         'user': \n",
    "             {'id': 25073877, \n",
    "             'id_str': '25073877',\n",
    "             'name': 'Donald J. Trump',\n",
    "             'screen_name': 'realDonaldTrump',\n",
    "             'location': 'Washington, DC',\n",
    "             'description': '45th President of the United States of AmericaðŸ‡ºðŸ‡¸',\n",
    "             'url': 'https://t.co/OMxB0x7xC5',\n",
    "             'entities':\n",
    "                 {'url': \n",
    "                     {'urls': [\n",
    "                         {'url': 'https://t.co/OMxB0x7xC5',\n",
    "                         'expanded_url': 'http://www.Instagram.com/realDonaldTrump',\n",
    "                         'display_url': 'Instagram.com/realDonaldTrump',\n",
    "                         'indices': [0, 23]}\n",
    "                         ]\n",
    "                      },\n",
    "                  'description': \n",
    "                      {'urls': []}\n",
    "                   },\n",
    "              'protected': False,\n",
    "              'followers_count': 46576912,\n",
    "              'friends_count': 45,\n",
    "              'listed_count': 83700,\n",
    "              'created_at': 'Wed Mar 18 13:46:38 +0000 2009',\n",
    "              'favourites_count': 24,\n",
    "              'utc_offset': -18000,\n",
    "              'time_zone': 'Eastern Time (US & Canada)',\n",
    "              'geo_enabled': True,\n",
    "              'verified': True,\n",
    "              'statuses_count': 36761,\n",
    "              'lang': 'en',\n",
    "              'contributors_enabled': False,\n",
    "              'is_translator': False,\n",
    "              'is_translation_enabled': True,\n",
    "              'profile_background_color': '6D5C18',\n",
    "              'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/530021613/trump_scotland__43_of_70_cc.jpg',\n",
    "              'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/530021613/trump_scotland__43_of_70_cc.jpg',\n",
    "              'profile_background_tile': True, \n",
    "              'profile_image_url': 'http://pbs.twimg.com/profile_images/874276197357596672/kUuht00m_normal.jpg',\n",
    "              'profile_image_url_https': 'https://pbs.twimg.com/profile_images/874276197357596672/kUuht00m_normal.jpg',\n",
    "              'profile_banner_url': 'https://pbs.twimg.com/profile_banners/25073877/1515478614',\n",
    "              'profile_link_color': '1B95E0',\n",
    "              'profile_sidebar_border_color': 'BDDCAD',\n",
    "              'profile_sidebar_fill_color': 'C5CEC0',\n",
    "              'profile_text_color': '333333',\n",
    "              'profile_use_background_image': True,\n",
    "              'has_extended_profile': False,\n",
    "              'default_profile': False,\n",
    "              'default_profile_image': False,\n",
    "              'following': False, 'follow_request_sent': False,\n",
    "              'notifications': False,\n",
    "              'translator_type': 'regular'},\n",
    "          'geo': None, \n",
    "          'coordinates': None,\n",
    "          'place': None,\n",
    "          'contributors': None,\n",
    "          'is_quote_status': False,\n",
    "          'retweet_count': 31599,\n",
    "          'favorite_count': 116375,\n",
    "          'favorited': False,\n",
    "          'retweeted': False, \n",
    "          'lang': 'en'}</code>\n",
    "<br><br>\n",
    "\n",
    "As you can see, whatever a Tweet is, it is complex thing. (Can you even find the text of the tweet that actually shows up on your tweetbox?) What is clear is it exists in a set of relationships. And if there are relationships, there must be things being related. So let's just call those things *objects* to be stay generic as possible. \n",
    "\n",
    "In object-oriented programming (OO), objects are a collection of *fields* (or attributes) and *methods*, procedures to modify the fields. The essense of an *OO* program is the design of the objects and manipulation of them via methods. (OK, and a bunch of stuff we won't get into.) To do something with objects, we need to know a lot about them, but if we are just moving them around as we are when collecting data, we just need the names of the fields and their contents. So we need a file format that can keep track of attributes of objects.\n",
    "\n",
    "Maybe just a standard spreadsheet (.csv, .xls) you're probably intimately familiar with could work? A row for each observation can work fine for keeping track of an observation and its attributes, but what if an attribute is a reference to something other than another observation of the same type? That is, to another type of object? There is a good chance we'll want to know more about those other objects so we want its attributes too. Unfortunately, a spreadsheet isn't very convenient for transporting that type of information. The JSON format can help us address this problem and that's why lots of data rich APIs use them.\n",
    "\n",
    "Going back the tweet above, you can see that this record of a single tweet contains references to lots of other objects, each with it's own attributes. They are encased in curly brackets {} and indented here for visual clarity. Examples are the posting user, \"entities\", and urls.\n",
    "\n",
    "It's having this type of data that makes big data interesting\\* to social scientists in my opinion. These data offer a chance to understand the rich, relational context of the observations. We can track them through time better too. The existence of this rich data can make managing all of it challenging (we have more than a spreadsheet can contain, but we also don't have a nice clean spreadsheet!). The benefits far outpace the problems!\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some package imports to get started ###\n",
    "<br>\n",
    "Next we need to import the core module with the `import twttr` statement below. We'll also import a package for displaying HTML within this notebook using the `from IPython.dispay import HTML` statement.\n",
    "<br>\n",
    "<br>\n",
    "If you haven't already installed the Twitter API package `python-twitter` and the search tool `whoosh` via Anaconda or pip/easy_install, you'll get an error until you do the install.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import twttr\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Before you can get started with collecting and analyzing, you'll need to authenticate to the Twitter API. The very first time you run the code in the cell below, you'll need to authenticate using your own Twitter account, so if you don't have one you'll want to register now. \n",
    "\n",
    "When you hit shift+enter to run the code the first time, you'll be taken to the Twitter website to sign in. It will then provide you with a PIN number to insert into the newly visible blue highlighted space in the cell above. Once you've done that, it will save your login data. You'll still need to run the login code everytime you start this notebook, but you won't need to authenticate via Twitter's webpage again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_connection = twttr.TWTTR()\n",
    "my_connection.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Getting Data Through the API ###\n",
    "\n",
    "Now we're all authenticated to the Twitter API and can start getting data. You probably don't want to just start getting the realtime feed, so let's start with something just to whet our appetite. Let's just find a user to start.\n",
    "The command `twttr.viewUser(\"user_handle\")` does that and reports some of the user's metadata. It also returns a string that allows you to render the results as HTML using the aptly named `HTML()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_connection.viewUser(\"KingJames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands `viewUser()` and `HTML` are not at all that important right here but you can see why we'll use them by running the following line of code that uses the actual Twitter API call `GetUser(screen_name=\"\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_connection.TWIT_PIPE.GetUser(screen_name=\"KingJames\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's Twitter's default JSON file for when you ask for a user. It's ease for a computer to pull that apart, but what a visual mess! And it is some very basic information and it responses can get much worse! Let's try the following.\n",
    "<br><br>\n",
    "## getUsersTweets( ) ##\n",
    "`getUsersTweets()` can get up to 200 of a user's most recent tweets. Below is an example of its use to get 25 tweets from ESPN's NBA account.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "the_tweets = my_connection.getUsersTweets(\"ESPNNBA\", count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this request contained a lot more than you see above. The tweets have been cleaned and reformated to make it easier and more enjoyable to quickly see what might be relevant to your research. The cell below has the direct Twitter API call if you want to see the mess that comes back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_connection.TWIT_PIPE.GetUserTimeline(screen_name=\"ESPNNBA\",count=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## index_Tweets()##\n",
    "\n",
    "After you've run the `getUsersTweets()` command, all of the cleaned Tweet objects are sitting in the list `the_tweets`. You can preview them in the output window beneath the command after you've run it, but if you want to save them for future analysis, you can index them using the `index_Tweets( )` command. This command creates a searchable database using the Python module `whoosh`. We'll cover how to do searchs later in this lab, but first we need to get them into the database so that we have something to search through!\n",
    "#### Important Note ####\n",
    "The seearchable index is saved in the folder named `twitter_index`, which is in the same folder as this notebook. Saving it allows the index to be maintained if you close this notebook or session. Once you start indexing the first time, all new indexing will add to the same index. If you'd like to start over, you'll need to clear and reset the index. The command for that is below. All entries will be lost so be careful!<br>\n",
    "`my_connection.clear_index()`<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_connection.index_Tweets(the_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This command does a lot of things behind the scenes, including running the text through the *Linguistic Inquiry and Word Count* package to create *sentiment* scores. We'll cover that topic later.<br>\n",
    "\n",
    "An important thing to know is that this command accepts a `batch` argument to expedite retrieving the collection of tweets you just added to the index. The index pools everything you put into it so the batch label is important if you want to track the different results from your queries. To use this functionality, simply provide a unique batch number whenever you run the command. You can then provide this number as an index search term later. Otherwise the default batch number is 1 and does not automatically increment, meaning all tweets will be in batch 1. I highly recommend noting the contents of the batches in somewhere such as one of the cells in this notebook. <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_connection.index_Tweets(the_tweets, batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tweetSearch(   ) ##\n",
    "Searching by users can be a great way to get data, but you'll probably also want to search by terms, hashtags, or even URLs. To do that, you use the `tweetSearch( )` method shown here. Twitter caps the number of tweets it will return at 100 which you might expect is a very small sample of all the possible tweets matching your search term(s). Twitter doesn't tell us much about how they determine the sample, but they do allow us to specify whether we want the most recent tweets, the most popular or a mixture of both. You can specify this with the second argument below; the options are `popular`, `recent`, or `mixed` and the default if you omit this argument is `recent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_curriculum_Tweets = my_connection.tweetSearch(\"#HiddenCurriculum\", \"recent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "You might notice a lot of repeated text in there. Those are all retweets, which of course count as tweets themselves.\n",
    "<br><br>\n",
    "Again, to ensure that you can look at and analyze these tweets later, you need to index them using the `index_Tweets( )` command. Below the indexing command is the Twitter API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_connection.index_Tweets(hidden_curriculum_Tweets, batch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_connection.TWIT_PIPE.GetSearch(term=\"#HiddenCurriculum\",result_type=\"recent\", count=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Search the index ##\n",
    "<br>\n",
    "The `tweetSearch( )` command searches Twitter for tweets related to the term you supplied, but there will unexplored context in those results. To explore that context, we index the tweets (metadata and all) and then search within our database. You use the `search( )` command to do this. `search( )` accepts arguments for any combination of fields stored in the index. For this particular index, those terms are as follows:\n",
    "\n",
    "  + `tweet_id`        - The unique integer that identifies this tweet\n",
    "  + `owner_name`      - The displayed name of the original poster (i.e. Chris Rock)\n",
    "  + `owner_sn`        - The screen name of the original poster (i.e. chrisrock)\n",
    "  + `owner_id`        - The Twitter ID of the original poster (i.e. 238319766)\n",
    "  + `batch`           - The batch number that you've been specifying. (default=1)\n",
    "  + `text`            - The content of the tweet itself\n",
    "  + `posted`          - The time the tweet was posted (GMT +0:00)\n",
    "  + `isRT`            - (True/False) Whether the tweet was a retweet\n",
    "  + `timesRT`         - The number of times the tweet was retweeted\n",
    "  + `timesFav`        - The number of times the tweet was favorited\n",
    "  + `hashtags`        - All the hashtags appearing in the tweet\n",
    "  + `mentions`        - All the mentions in the tweet\n",
    "  + `media`           - The included media type, if applicable\n",
    "  \n",
    "These arguments are entered all together in a single string, a series of character within double quotation marks. We'll go over the details of the syntax shortly.\n",
    "  \n",
    "In addition to these index arguments, you can search by a period of the day using the regular `time_slice` argument. The argument looks like this: `[\"09:00\":\"17:00\"]`. The period starts at the time on the left and ends on the time on the right and it is important that there are two numbers on each side of the colon. If the starting time is later than the ending time, it searches past midnight and into the next day.\n",
    "\n",
    "You can also limit the number of tweets returned using the regular `limit` argument. The default is 100.\n",
    "\n",
    "Finally, you can save your search results for viewing as an HTML page later using the `saveAs` argument. The default name is `search`\n",
    "<br>\n",
    "Below are some examples of proper syntax. All of these variants can be combined into a single command but are in separate commands here for visual clarity.\n",
    "\n",
    "<br><br><br>\n",
    "Here is an example looking at the hashtags and posting times. We'll address the data the command returns later in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, data, display, tweet_list = my_connection.search(\"hashtags:HiddenCurriculum posted:'July 10 to July 28'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of multiple quotation marks. **Any argument in that appears in the bulleted list above must appear within the double quotation marks**. There are also single quotation marks for the values that follow the key terms. Also note that there is no comma between key terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, data, display, tweet_list = my_connection.search(\"'academic' isRT:'True'\", saveAs=\"HiddenCurric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *timeRT* and *timesFav* key terms allow you to find the most popular tweets (or the least or in between).  Whether there is an important difference between favoriting or retweeting from the prospective of users is not clear to me, but you have both at your disposal. <br><br>\n",
    "Here are additional examples of search queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, data, display, tweet_list = my_connection.search(\"content:'hidden' mentions:'JessicaCalarco'\", limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one gets up to 100 tweets made between the hours of 10pm and 3am and save the results in a file named LateNight_tweets.html in the folder this notebook is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, data, display, tweet_list = my_connection.search(\"batch:'2' posted:'Jul 1 to Jul 29'\", time_slice=[\"22:00\",\"03:00\"], saveAs=\"LateNight_tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below searches the text of the tweets for the variations of a phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, data, display, tweet_list = my_connection.search(\"'Hidden' OR 'HiddenCurriculum' OR '#HiddenCurriculum'\",saveAs=\"Curriculum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one searches for tweets that are either tagged with a search term or mentions a user's offical Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, data, display, tweet_list = my_connection.search(\"hashtags:'HiddenCurriculum' OR mentions:'JessicaCalarco'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember #### \n",
    "All of these search terms can be used simultaneously. Once you get the hang of it, you can do some very powerful sorting. The syntax can be finicky so if you get 0 results and you're expecting some, make sure there aren't extra commas or spaces and that the quotation marks are always in pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What search( ) does####\n",
    "Running the search command does a lot of things behind the scenes. It packages information up for you in the terms that appear on the left hand side of the equals sign. \n",
    "The first term is named `results` but you can name it whatever you'd like by replacing `results` with your term before running the command. (You can do this for all terms by the way.) Results is an object that can be used to display the averages of the individual LIWC values for the whole group of Tweets. \n",
    "`data` is the raw information that can be used to find differences between LIWC averages from different search results, a topic covers below. \n",
    "`display` can be used to view the individual tweets in the search results in a HTML table. \n",
    "Finally, `tweet_list` is a list of the Tweet IDs for the tweets in the search results.\n",
    "<br>\n",
    "<br>\n",
    "The other thing that the command does is produce an HTML file with the Tweets represented as circles proportional to the number of times it was retweeted. An example of the link can be found [here.](files/tools/tweetpack.html) Your search results show up in the folder this notebook is in under the name you specified with the `saveAs` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Inspecting Retweets##\n",
    "What you've been collecting and looking at up until now are just a handful of tweets but you might really want to look at what happens to the tweet after the original poster submitted it. Because of Twitter's rate-limiting policies, this can be a time consuming task and accordingly the indexing command doesn't collect those data immediately. Rather, you need to tell the index which ones to collect using the `inspectRetweets( )` command. The argument is just a list of tweet ids, most likely gotten from the `search( )` command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = min(5,len(tweet_list))\n",
    "\n",
    "my_connection.inspectRetweets(tweet_list[:ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command can take a long time to run and the amount of time depends on two things. The first is the number of tweets; the more you have in the list argument, the longer it will take. The second is whether or not the tweets are from the same person. The code looks up the poster's friends and followers to determine relationship the person has to the retweeters and saves the data. This process is slow (in the world of computers, that is) so if you are looking at posts from a bunch a people, it can take a long time. So if you have a stack of tweets from Oprah, it might take a couple of minutes. But if you have the most popular Tweets about the latest Star Wars movie, you could get dinner and it might not be finished.\n",
    "<br><br>\n",
    "### What Inspecting Retweets Does###\n",
    "Inspecting the retweets looks at the who retweeted the original, when they did and the nature of their relationship to the original poster. It also summarizes these data and visualizes the network of the retweets in a webpage reachable by clicking on the tweet in search interface. The circle representing the tweet will turn blue if you've inspected it.  You can then click on it and be taken to the page with more details. \n",
    "\n",
    "<br>\n",
    "This page includes a visualization of the network in which the length of the ties is proportional to the number of hours after the original tweet that the retweet was posted. The nodes are also colored according to the retweeter's relationship with the original poster. An important thing to note about these data is that Twitter limits the information to a random sampling of 100 of the retweets. So while the tweet might have 2000 retweets, you'll see at most 101 nodes in this network. For the purpose of this lab, you may consider this a representative sample and conduct your analyze using these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking: Viewing Results ##\n",
    "\n",
    "When you run a search, you can see the titles of the posts that the query found but it is actually spitting back much more information. We can look at those using the data object `results`, `data`, and `display`.\n",
    "<br>\n",
    "While it appears last, let's start with `display` first. It contains all of the raw information from the posting returned by the query. To make it easier to look at, it is packaged up as an HTML file that we can look at by running the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to notice about this:\n",
    "\n",
    "1) That is a lot of information in our poor little Notebook. If you'd rather not look at it here, don't run the command and just go find the file named 'search_results.html' in your CL folder. You can even just keep that file open in your browser and refresh it after new searchs. Doing so will make your notebook easier to use and the HTML also renders better there.\n",
    "\n",
    "2) There is big table at the bottom of each post. These contain the textual analysis of the post using the LIWC dictionary. It contains basic statistics like word counts (WC) and the number of pronouns but also the frequency of words related to positive emotions (posemo) and tentativeness (tentat). The full names and categories for the terms are available on the LIWC site [here](http://www.liwc.net/descriptiontable1.php)\n",
    "\n",
    "3) The tables aren't the same for each post. That's because some posted scored zero on the LIWC count and they've been omitted in order to condense the information. \n",
    "\n",
    "4) These tables are really good for exploring patterns worth looking into more, but chances are you aren't going to want to find averages for these tables by hand. Thats what the `results` object does for you.<br>\n",
    "\n",
    "## Viewing Group Statistics##\n",
    "The *results* object contains useful information about that group of search results, namely the average, the standard deviation, the maximum value and the minimum value for each of the LIWC categories. You can display it as HTML using the command below or via the file *search_averages.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `data` object contains the raw numbers of the results. These can be used to compare different search groups and create smaller tables of more relevant data. To do that, you need to specify two different searches and give the outputs different names as is shown in the next two lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1, data_1, display_1, tweet_list_1 = my_connection.search(\"batch:2 content:'women'\", saveAs=\"morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2, data_2, display_2, tweet_list_2 = my_connection.search(\"batch:2 content:'men'\", saveAs=\"afternoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then put the two data objects, 'data_1' and 'data_2' into the 'LIWC_differences( )' command to get a table back. You can then display it by passing it to the 'HTML( )' command. The two lines below do it for the searches above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = my_connection.LIWC_differences(data_1, data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like to trim this big table to just the relevant information, run the 'LIWC_differences_subset( )' command with a list of the terms you want. This is a handy thing for producing actual results to be included somewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = my_connection.LIWC_differences_subset(data_1, data_2, ['WC', 'posemo', 'negemo', 'swear', 'anger', 'affect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
